from langchain.tools import BaseTool
import requests
from bs4 import BeautifulSoup
import time
from typing import Optional

class NewsSearchTool(BaseTool):
    name = "news_search_tool"
    description = "æœç´¢ä¸Žè‚¡ç¥¨æˆ–è¡Œä¸šç›¸å…³çš„æœ€æ–°æ–°é—»å’Œå¸‚åœºæƒ…ç»ªã€‚è¾“å…¥ï¼šè‚¡ç¥¨ä»£ç æˆ–å…³é”®è¯"
    
    # ç¼“å­˜
    _cache = {}
    _cache_ttl = 600  # 10åˆ†é’Ÿç¼“å­˜ï¼ˆæ–°é—»æ›´æ–°è¾ƒæ…¢ï¼‰
    
    def _get_cached_data(self, query: str) -> Optional[list]:
        if query in self._cache:
            data, timestamp = self._cache[query]
            if time.time() - timestamp < self._cache_ttl:
                return data
        return None
    
    def _set_cache(self, query: str, data: list):
        self._cache[query] = (data, time.time())
    
    def _run(self, query: str) -> str:
        try:
            query = query.strip().upper()
            
            # æ£€æŸ¥ç¼“å­˜
            cached = self._get_cached_data(query)
            if cached:
                return self._format_output(query, cached, from_cache=True)
            
            # èŽ·å–æ–°é—»æ•°æ®
            news_items = self._fetch_news(query)
            
            if not news_items:
                return f"âš ï¸ æœªæ‰¾åˆ°ä¸Ž '{query}' ç›¸å…³çš„è¿‘æœŸæ–°é—»"
            
            # å­˜å…¥ç¼“å­˜
            self._set_cache(query, news_items)
            
            return self._format_output(query, news_items)
            
        except Exception as e:
            return f"âŒ æœç´¢æ–°é—»æ—¶å‡ºé”™: {str(e)}"
    
    def _fetch_news(self, query: str, max_items: int = 5) -> list:
        """
        èŽ·å–æ–°é—»æ•°æ®
        è¿™é‡Œä½¿ç”¨ Google Finance ä½œä¸ºå…è´¹æ¥æº
        ä¹Ÿå¯ä»¥æ›¿æ¢ä¸º NewsAPI, Finnhub ç­‰ä»˜è´¹æœåŠ¡
        """
        news_items = []
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨ Google Finance (å…è´¹ä½†å¯èƒ½è¢«é™æµ)
            url = f"https://www.google.com/finance/quote/{query}:NASDAQ"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # å°è¯•æå–æ–°é—»æ ‡é¢˜
                # æ³¨ï¼šGoogle Finance çš„ HTML ç»“æž„å¯èƒ½å˜åŒ–ï¼Œè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹
                news_sections = soup.find_all('div', class_='yY3Lee')
                
                for section in news_sections[:max_items]:
                    try:
                        title = section.get_text(strip=True)
                        if title and len(title) > 10:
                            news_items.append({
                                'title': title,
                                'source': 'Google Finance',
                                'sentiment': self._analyze_sentiment(title)
                            })
                    except:
                        continue
            
            # å¦‚æžœ Google Finance æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if not news_items:
                news_items = self._get_fallback_news(query)
            
        except Exception as e:
            # å¤±è´¥æ—¶è¿”å›žæ¨¡æ‹Ÿæ•°æ®ï¼ˆå®žé™…éƒ¨ç½²æ—¶åº”ç§»é™¤ï¼‰
            news_items = self._get_fallback_news(query)
        
        return news_items
    
    def _get_fallback_news(self, query: str) -> list:
        """
        å¤‡ç”¨æ–°é—»æºï¼ˆç¤ºä¾‹ï¼‰
        å®žé™…ä½¿ç”¨æ—¶åº”è¯¥é›†æˆçœŸå®žçš„ APIï¼Œå¦‚ï¼š
        - NewsAPI (å…è´¹é¢åº¦: 100 requests/day)
        - Finnhub (å…è´¹é¢åº¦: 60 requests/min)
        - Alpha Vantage News API
        """
        # è¿™é‡Œè¿”å›žæ¨¡æ‹Ÿæ•°æ®ï¼Œæç¤ºç”¨æˆ·é…ç½®çœŸå®žAPI
        return [
            {
                'title': f'{query} çš„æ–°é—»æ•°æ®éœ€è¦é…ç½®ä¸“ä¸šAPI',
                'source': 'ç³»ç»Ÿæç¤º',
                'sentiment': 'neutral'
            },
            {
                'title': 'å»ºè®®é›†æˆ NewsAPI æˆ– Finnhub ä»¥èŽ·å–å®žæ—¶æ–°é—»',
                'source': 'ç³»ç»Ÿæç¤º',
                'sentiment': 'neutral'
            },
            {
                'title': 'å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®',
                'source': 'ç³»ç»Ÿæç¤º',
                'sentiment': 'neutral'
            }
        ]
    
    def _analyze_sentiment(self, text: str) -> str:
        """
        ç®€å•çš„æƒ…ç»ªåˆ†æžï¼ˆåŸºäºŽå…³é”®è¯ï¼‰
        å®žé™…åº”ç”¨å¯ä½¿ç”¨ NLTK, TextBlob æˆ– FinBERT
        """
        text_lower = text.lower()
        
        # æ­£é¢è¯æ±‡
        positive_words = [
            'up', 'rise', 'gain', 'profit', 'growth', 'surge', 'rally',
            'beat', 'exceed', 'record', 'high', 'strong', 'bullish',
            'ä¸Šæ¶¨', 'å¢žé•¿', 'ç›ˆåˆ©', 'çªç ´', 'åˆ›æ–°é«˜', 'å¼ºåŠ²'
        ]
        
        # è´Ÿé¢è¯æ±‡
        negative_words = [
            'down', 'fall', 'loss', 'drop', 'decline', 'crash', 'plunge',
            'miss', 'weak', 'concern', 'risk', 'bearish', 'cut',
            'ä¸‹è·Œ', 'ä¸‹é™', 'äºæŸ', 'æš´è·Œ', 'é£Žé™©', 'å‰Šå‡'
        ]
        
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return 'positive'
        elif neg_count > pos_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _format_output(self, query: str, news_items: list, from_cache: bool = False) -> str:
        """æ ¼å¼åŒ–è¾“å‡º"""
        cache_note = " [ç¼“å­˜æ•°æ®]" if from_cache else ""
        
        output = f"""
ðŸ“° **{query} å¸‚åœºæƒ…ç»ªä¸Žæ–°é—»**{cache_note}

"""
        
        # ç»Ÿè®¡æƒ…ç»ª
        sentiments = [item['sentiment'] for item in news_items]
        positive_count = sentiments.count('positive')
        negative_count = sentiments.count('negative')
        neutral_count = sentiments.count('neutral')
        
        total = len(sentiments)
        if total > 0:
            output += f"**æ•´ä½“æƒ…ç»ªåˆ†å¸ƒï¼š**\n"
            output += f"- ðŸŸ¢ æ­£é¢: {positive_count}/{total} ({positive_count/total*100:.0f}%)\n"
            output += f"- ðŸ”´ è´Ÿé¢: {negative_count}/{total} ({negative_count/total*100:.0f}%)\n"
            output += f"- ðŸŸ¡ ä¸­æ€§: {neutral_count}/{total} ({neutral_count/total*100:.0f}%)\n\n"
            
            # ç»¼åˆæƒ…ç»ªåˆ¤æ–­
            if positive_count > negative_count * 1.5:
                output += "ðŸ“Š **å¸‚åœºæƒ…ç»ªï¼š** ðŸŸ¢ åä¹è§‚\n\n"
            elif negative_count > positive_count * 1.5:
                output += "ðŸ“Š **å¸‚åœºæƒ…ç»ªï¼š** ðŸ”´ åæ‚²è§‚\n\n"
            else:
                output += "ðŸ“Š **å¸‚åœºæƒ…ç»ªï¼š** ðŸŸ¡ ä¸­æ€§/åˆ†æ­§\n\n"
        
        # æ˜¾ç¤ºæ–°é—»æ ‡é¢˜
        output += f"**æœ€è¿‘æ–°é—» (å‰{len(news_items)}æ¡):**\n\n"
        
        for i, item in enumerate(news_items, 1):
            sentiment_emoji = {
                'positive': 'ðŸŸ¢',
                'negative': 'ðŸ”´',
                'neutral': 'ðŸŸ¡'
            }
            emoji = sentiment_emoji.get(item['sentiment'], 'ðŸŸ¡')
            
            output += f"{i}. {emoji} {item['title']}\n"
            output += f"   æ¥æº: {item['source']}\n\n"
        
        # æŠ•èµ„æç¤º
        output += "ðŸ’¡ **æŠ•èµ„å»ºè®®ï¼š**\n"
        if positive_count > negative_count * 1.5:
            output += "- å¸‚åœºæƒ…ç»ªç§¯æžï¼Œä½†éœ€ç»“åˆåŸºæœ¬é¢å’ŒæŠ€æœ¯é¢ç»¼åˆåˆ¤æ–­\n"
            output += "- æ³¨æ„æ˜¯å¦å­˜åœ¨è¿‡åº¦ä¹è§‚ï¼Œè­¦æƒ•è¿½é«˜é£Žé™©\n"
        elif negative_count > positive_count * 1.5:
            output += "- å¸‚åœºæƒ…ç»ªåæ‚²è§‚ï¼Œå¯èƒ½å­˜åœ¨ææ…Œæ€§æŠ›å”®\n"
            output += "- è‹¥åŸºæœ¬é¢è‰¯å¥½ï¼Œå¯èƒ½æ˜¯é€¢ä½Žä¹°å…¥çš„æœºä¼š\n"
        else:
            output += "- å¸‚åœºæƒ…ç»ªåˆ†æ­§ï¼Œå»ºè®®è°¨æ…Žè§‚æœ›\n"
            output += "- ç­‰å¾…æ›´æ˜Žç¡®çš„æ–¹å‘æ€§ä¿¡å·\n"
        
        return output.strip()
    
    async def _arun(self, query: str) -> str:
        return self._run(query)
